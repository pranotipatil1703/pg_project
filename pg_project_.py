# -*- coding: utf-8 -*-
"""PG_Project .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-YeeMIaIF2D4Tdml419U2xmaiMVS4LdT

# Data Preprocessing

1. Importing The Required Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""1.2. Importing The Dataset"""

df=pd.read_excel("default of credit card clients.xls")
df.head(10)

df.tail()

"""# 1.3. Understanding the Dataset

1. LIMIT_BAL:
Continuous:Credit limit of the person
2. SEX:
Categorical:
1-male
2-female
3. EDUCATION:
Categorical:
1-graduate school
2-university
3-high school
4-others
5-Unknown
6-Unknown
4. MARRIAGE:
Categorical:
1-married
2-unmarried
3-others
0-Unknown
5. AGE:
Continuous:age of the card holder
6. PAY_0-PAY_6:
Categorical:History of the past monthly payment records from April 2005 to September
2005
"0"-No due payment
"-1"-One due payment
"-2"-Two due payments
7. BILL_AMT1-BILL_AMT6:
Continuous:Amount of Bill Statements
8. PAY_AMT1-PAY_AMT6:
Continuous:Amount of Previous Payments
9. default payment next month:
Categorical:Whether person is going to default or not
1-Default
0-Not Defaul

# 1.4. Changing The Name of Column
"""

df.rename(columns={"PAY_0":"PAY_1"},inplace=True)

df.rename(columns={"default payment next month":"Default"},inplace=True)

df.head()

"""# 1.5. Cleaning EDUCATION column

- The variable ‘EDUCATION’ has three categories with similar inform
ation:
- 4: others, 5: unknown, and 6: unknown 0:unknown
-We need to group this three different categaries into one single c
ategory
"""

df["EDUCATION"].unique()

df["EDUCATION"]=np.where(df["EDUCATION"]==0, 4, df["EDUCATION"])
df["EDUCATION"]=np.where(df["EDUCATION"]==5, 4, df["EDUCATION"])
df["EDUCATION"]=np.where(df["EDUCATION"]==6, 4, df["EDUCATION"])

df["EDUCATION"].unique()

"""# 1.6. Cleaning MARRIAGE Column

- Similarly, the column 'marriage' should have three categories: 1
= married, 2 = single, 3 = others but it contains a category '0'
which will be joined to the category '3'.
"""

df["MARRIAGE"]=np.where(df["MARRIAGE"]==0, 3, df["MARRIAGE"])
df["MARRIAGE"].unique()

print("Number of Rows & Columns:",df.shape)

"""# 1.7. Checking The Datatypes"""

df.dtypes

"""# 1.8. Checking the NULL Values"""

df.isnull().sum()

df.info()

"""# 1.9. Understanding the
Minimum,Maximum,Mean,Median,Mode,Quartiles
"""

df.describe()

"""# 1.10. Understanding the Correlation Between The Dataset"""

df.corr()

"""# Part2-EDA(Exploratory Data Analysis)

# 2.1. Finding The Total Defaulters
"""

#Count of Defaulters
defaulting=df.Default.sum()
not_defaulting=len(df)-defaulting
print("Total Number of Defaulters:",defaulting)
print("Total Number of Non-Defaulters:",not_defaulting)
print("--------------------------------------------------------")
#Percentage of Defaulters
deafulter_Perc=defaulting/len(df)*100
Nondeafulter_Perc=not_defaulting/len(df)*100
print("Total Percent of Defaulters:",deafulter_Perc,"%")
print("Total Percenr of Non-Defaulters:",Nondeafulter_Perc,"%")
print("--------------------------------------------------------")
#Plotting the Piechart
pie1=np.array([deafulter_Perc,Nondeafulter_Perc])
mylabels=["Deafulters","Not-Defaulters"]
myexplode=[0.2,0]
plt.pie(pie1,labels=mylabels,shadow=True,startangle = 90,explode=myexplode,autopct="%.2f%%")
plt.legend()
plt.title("Percentage of Defaulters & Non-Defaulters")
plt.show()

"""# 2.2. Plotting The Heatmap to Understand The Correlation Between Cloumns"""

plt.figure(figsize=(10,10))
corr = df.corr()
sns.heatmap(corr, cmap="Reds",linewidths=.5)

"""# 2.3. Finding The Defaulters Based On Limit Balance Of the Card Holder"""

x1 = list(df[df['Default'] == 1]['LIMIT_BAL'])
x2 = list(df[df['Default'] == 0]['LIMIT_BAL'])
plt.figure(figsize=(12,4))
plt.hist([x1,x2],bins=40)
plt.xlim([0,700000])
plt.legend(['Defaulter', 'Non-Defaulter'], title = 'Default', loc='upper right', facecolor='black')
plt.grid(color="pink",axis="y")
plt.xlabel("LIMIT BALANCE OF CARD HOLDER")
plt.ylabel("FREQUENCY")
plt.title("LIMIT BALANCE HISTOGRAM BY TYPE OF CREDIT CARD")
plt.show()

"""# 2.4. Finding The Linear Realtionship Between The indepedent Variables Using The ScatterPlot"""

sns.scatterplot(data=df,x=df["BILL_AMT1"],y=df["BILL_AMT6"],hue="Default",
palette="Set2")
plt.title("Bill Amount1 vs Bill Amount6")
plt.show()

sns.scatterplot(data=df,x=df["BILL_AMT2"],y=df["BILL_AMT4"],hue="Default",
palette="Set2")
plt.title("Bill Amount2 vs Bill Amount4")
plt.show()

sns.scatterplot(data=df,x=df["PAY_AMT2"],y=df["PAY_AMT4"],hue="Default",palette="Set2")
plt.title("Payment Amount2 vs Payment Amount4")
plt.show()

"""# 2.5. Plotting The BoxPlots Using Different Variables"""

df1=df[["PAY_AMT1","PAY_AMT2","PAY_AMT3","PAY_AMT4","PAY_AMT5","PAY_AMT6"]]
sns.boxplot(data=df1)
plt.ylim([-20000,200000])
plt.title("Boxplot")
plt.xticks(rotation=90)
plt.show()

df2=df[["BILL_AMT1","BILL_AMT2","BILL_AMT3","BILL_AMT4","BILL_AMT5","BILL_AMT6"]]
sns.boxplot(data=df2)
plt.ylim([-200000,1000000])
plt.title("Boxplot")
plt.xticks(rotation=90)
plt.show()

sns.boxplot(data=df,x="MARRIAGE",y="LIMIT_BAL",hue="SEX")
sns.set_style("darkgrid")
plt.ylim([0,700000])
plt.title("MARRIAGE VS LIMIT BALANCE GENDERWISE")
plt.show()

sns.boxplot(data=df,x="EDUCATION",y="LIMIT_BAL",hue="Default")
sns.set_style("darkgrid")
plt.ylim([0,700000])
plt.title("EDUCATION VS LIMIT BALANCE GENDERWISE")
plt.show()

"""# 2.6. Plotting The Bar Chart To Find Defaulters Based On Sex,Education,Marriage"""

f, axes = plt.subplots(1, 3, figsize=(15,5))
sns.countplot(data=df,x="SEX",hue="Default",ax=axes[0],palette="Set2")
sns.countplot(data=df,x="EDUCATION",hue="Default",ax=axes[1],palette="husl")
sns.countplot(data=df,x="MARRIAGE",hue="Default",ax=axes[2],palette="Spectral")
plt.suptitle("SEX-EDUCATION-MARRIAGE VS DEFAULT")
plt.show()

"""# 2.7. Plotting The Bar Charts To Find Out The Defaulters Based On The PAYMENT"""

f, axes = plt.subplots(2, 3, figsize=(15, 10))
f.suptitle('FREQUENCY OF CATEGORICAL VARIABLES (BY TARGET)')
sns.countplot(x="PAY_1", hue="Default", data=df, palette="pastel", ax=axes[0,0])
sns.countplot(x="PAY_2", hue="Default", data=df, palette="blend:#7AB,#EDA", ax=axes[0,1])
sns.countplot(x="PAY_3", hue="Default", data=df, palette="Spectral", ax=axes[0,2])
sns.countplot(x="PAY_4", hue="Default", data=df, palette="husl", ax=axes[1,0])
sns.countplot(x="PAY_5", hue="Default", data=df, palette="flare", ax=axes[1,1])
sns.countplot(x="PAY_6", hue="Default", data=df, palette="Set2", ax=axes[1,2])

"""# Part3-Scaling The Data Using StandardScaler"""

x=df.drop(labels=["Default"],axis=1)
y=df["Default"]

x.head()

y.head()

"""# 3.2. Splitting the Dataset into Training Dataset and Testing Dataset"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.7,random_state=0)

x_train

x_test

y_train

y_test

"""# 3.3. Scaling the Training Dataset Using StandardScaler"""

from sklearn.preprocessing import StandardScaler
train_scaler=StandardScaler()
test_scaler=StandardScaler()

scaled_train_data=train_scaler.fit_transform(x_train)
scaled_train_data=pd.DataFrame(data=scaled_train_data,columns=x_train.columns,index=x_train.index)
scaled_train_data

"""# 3.4. Scaling the Testing Dataset StandardScaler"""

pd.set_option("display.max_columns",30)
scaled_test_data=test_scaler.fit_transform(x_test)
scaled_test_data=pd.DataFrame(data=scaled_test_data,columns=x_test.columns,index=x_test.index)
scaled_test_data

"""# Part4-Feature Selection Using mutual_info_classifier"""

from sklearn.feature_selection import mutual_info_classif
info_gain1=mutual_info_classif(scaled_train_data,y_train)
info_gain1

data={"Column_Name":x_train.columns,"Mutual_Information":info_gain1}
gain1=pd.DataFrame(data)
gain1

sns.barplot(data=gain1,x="Column_Name",y="Mutual_Information",palette="Set2",order=gain1.sort_values("Mutual_Information",ascending = False).Column_Name)
plt.xticks(rotation=90)
plt.title("Column Wise Mutual Information")
plt.grid(color="black",axis="y")
plt.show()

"""# 4.2. Selecting the Top-11 Features for the Model Building"""

from sklearn.feature_selection import SelectKBest
eleven_col=SelectKBest(mutual_info_classif,k=11)
eleven_col.fit(scaled_train_data,y_train)
scaled_train_data.columns[eleven_col.get_support()]

"""# 4.3. Training Dataset"""

scaled_train_fs_data=eleven_col.transform(scaled_train_data)
scaled_train_fs_data=pd.DataFrame(scaled_train_fs_data,columns=scaled_train_data.columns[eleven_col.get_support()],index=scaled_train_data.index)
scaled_train_fs_data

"""# 4.4. Testing Dataset"""

scaled_test_fs_data=eleven_col.transform(scaled_test_data)
scaled_test_fs_data=pd.DataFrame(scaled_test_fs_data,columns=scaled_test_data.columns[eleven_col.get_support()],index=scaled_test_data.index)
scaled_test_fs_data

"""# Part5-Applying Machine Learning Algorithms

# 5.1. Logistic Regression
"""

import sys
from sklearn.linear_model import LogisticRegression
model_lr=LogisticRegression()
model_lr.fit(scaled_train_fs_data,y_train)
y_pred_logreg=model_lr.predict(scaled_test_fs_data)

info={"Actual Value":y_test,
 "Predicted Value":y_pred_logreg}
logreg=pd.DataFrame(info)
logreg.tail(18)

"""# 5.2. SVM(Support Vector Machine)"""

from sklearn.svm import SVC
model_svm=SVC()
model_svm.fit(scaled_train_fs_data,y_train)
y_pred_svm=model_svm.predict(scaled_test_fs_data)

info={"Actual Value":y_test,"Predicted Value":y_pred_svm}
svm=pd.DataFrame(info)
svm.sample(18)

"""# 5.3. Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
model_navbay=GaussianNB()
model_navbay.fit(scaled_train_fs_data,y_train)
y_pred_navbay=model_navbay.predict(scaled_test_fs_data)

info={"Actual Value":y_test,"Predicted Value":y_pred_navbay}
NB=pd.DataFrame(info)
NB.sample(18)

"""# 5.4. KNN(K-Nearest Neighbours)"""

from sklearn.neighbors import KNeighborsClassifier
model_knn=KNeighborsClassifier()
model_knn.fit(scaled_train_fs_data,y_train)
y_pred_KNN=model_knn.predict(scaled_test_fs_data)

info={"Actual Value":y_test,"Predicted Value":y_pred_KNN}
KNN=pd.DataFrame(info)
KNN.sample(18)

"""# 5.5. Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
model_dt=DecisionTreeClassifier()
model_dt.fit(scaled_train_fs_data,y_train)
y_pred_DT=model_dt.predict(scaled_test_fs_data)

info={"Actual Value":y_test,"Predicted Value":y_pred_DT}
DT=pd.DataFrame(info)
DT.sample(18)

"""# 5.6. Random Forest"""

from sklearn.ensemble import RandomForestClassifier
model_rf=RandomForestClassifier(n_estimators=100)
model_rf.fit(scaled_train_fs_data,y_train)
y_pred_RF=model_rf.predict(scaled_test_fs_data)

# info={"Actual Value":y_test,"Predicted Value":y_pred_RF}
RF=pd.DataFrame(info)
RF.sample(18)

"""# 5.7. XG Boost(eXtreme Gradient Boosting)"""

from xgboost import XGBClassifier
model_xgb=XGBClassifier(n_estimators=500)
model_xgb.fit(scaled_train_fs_data,y_train)
y_pred_XGB=model_xgb.predict(scaled_test_fs_data)

info={"Actual Value":y_test,"Predicted Value":y_pred_XGB}
XGB=pd.DataFrame(info)
XGB.sample(18)

"""# 5.8.ANN(Artificial Neural Network) Using TensorFlow & Keras"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model_ann=Sequential()
model_ann.add(Dense(11,input_dim=11,activation="sigmoid"))
model_ann.add(Dense(11,activation="relu"))
model_ann.add(Dense(11,activation="relu"))
model_ann.add(Dense(11,activation="relu"))
model_ann.add(Dense(1,activation="sigmoid"))
model_ann.compile(optimizer="Adam",loss="binary_crossentropy",metrics=["accuracy"])
model_ann.fit(scaled_train_fs_data,y_train,epochs=100,verbose=1)

acc=model_ann.evaluate(scaled_test_fs_data,y_test)
acc

y_pred_ann=model_ann.predict(scaled_test_fs_data)

"""# Part6-Evaluating The Performance of Algorithms

# 6.1. Accuracy Score
"""

from sklearn.metrics import accuracy_score
lr_as=accuracy_score(y_test,y_pred_logreg)
svm_as=accuracy_score(y_test,y_pred_svm)
nb_as=accuracy_score(y_test,y_pred_navbay)
knn_as=accuracy_score(y_test,y_pred_KNN)
dt_as=accuracy_score(y_test,y_pred_DT)
rf_as=accuracy_score(y_test,y_pred_RF)
xgb_as=accuracy_score(y_test,y_pred_XGB)
print("Accuracy_score-Logistic Regression:",lr_as)
print("Accuracy_score-SVM:",svm_as)
print("Accuracy_score-Naive Bayes:",nb_as)
print("Accuracy_score-KNN:",knn_as)
print("Accuracy_score-Decision Tree:",dt_as)
print("Accuracy_score-Random Forest:",rf_as)
print("Accuracy_score-XGBoost:",xgb_as)
print("Accuracy_score-ANN:",acc[1])

"""# 6.2. F1_Score"""

from sklearn.metrics import f1_score
lr_fs=f1_score(y_test,y_pred_logreg,average="weighted")
svm_fs=f1_score(y_test,y_pred_svm,average="weighted")
nb_fs=f1_score(y_test,y_pred_navbay,average="weighted")
knn_fs=f1_score(y_test,y_pred_KNN,average="weighted")
dt_fs=f1_score(y_test,y_pred_DT,average="weighted")
rf_fs=f1_score(y_test,y_pred_RF,average="weighted")
xgb_fs=f1_score(y_test,y_pred_XGB,average="weighted")
print("F1_Score-Logistic Regression:",lr_fs)
print("F1_Score-SVM:",svm_fs)
print("F1_Score-Naive Bayes:",nb_fs)
print("F1_Score-KNN:",knn_fs)
print("F1_Score-Decision Tree:",dt_fs)
print("F1_Score-Random Forest:",rf_fs)
print("F1_Score-XGBoost:",xgb_fs)

"""# 6.3. Creating The Dataframe Of the Performance Metrics"""

data={"Accuracy Score":[lr_as,svm_as,nb_as,knn_as,dt_as,rf_as,xgb_as,acc[1]], "F1_Score":[lr_fs,svm_fs,nb_fs,knn_fs,dt_fs,rf_fs,xgb_fs,acc[1]]}
table1=pd.DataFrame(data,index=["Logistic Regression","Support Vector Machine","Naive Bayes","K-Nearest Neighbour","Decision Tree","Random Forest","XG Boost","ANN"])
table1

"""# 6.4. Plotting The Performance Of All The Algorithms Using Line Chart"""

sns.lineplot(data = table1, markers=True, dashes=True)
plt.xticks(rotation=90)
plt.grid(color="black")
plt.xlabel("Machine Learning Algorithms Performance")
plt.ylabel("Score")
plt.title("Comparing the Accuracy & F1 Scores of different Algorithms")
plt.show()

"""# CONCLUSION

# After performing 8 algorithms we can clearly conclude that out of nine algorithms following 2 are the best performing algorithms
1. SVM(Support Vector Machine)
2. ANN(Artificial Neural Network)
"""